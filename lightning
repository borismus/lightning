#!/usr/bin/env python
import boto
import os
import pickle
import datetime
import logging
import markdown2
import re
import shutil
import signal
import sys
import time
import threading
import yaml

from boto.s3.key import Key
from distutils import dir_util
from jinja2 import Template
from jinja2 import Environment, FileSystemLoader
from livereload import Server, shell
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler


PREVIEW_ADDR = '127.0.0.1'
PREVIEW_PORT = 8000

DEFAULT_TIME = datetime.time(9, 0)
PWD = os.getcwd()

should_deploy = False


class BuildException(Exception):
  def __init__(self, error):
    self.error = error

  def __str__(self):
    return self.error


class SiteBuilder:

  def __init__(self, include_staging=True):
    # Parse lightning.yaml.
    f = open('lightning.yaml')
    build = yaml.load(f)
    # Get the roots.
    if not os.path.exists(build['template']):
      raise Exception('Invalid template root. Fix lightning.yaml')
    self.template_root = build['template']
    if not os.path.exists(build['content']):
      raise Exception('Invalid content root. Fix lightning.yaml')
    self.content_root = build['content']
    self.staging_root = os.path.join(self.content_root, 'staging')
    self.output_root = include_staging and build['output'] or \
        build['output_deploy']

    # Figure out deploy buckets.
    self.deploy = 'deploy' in build and build['deploy']

    # Figure out the pathes.
    self.archive_path = os.path.join(self.output_root, '.archived/')
    self.cache_path = os.path.join(self.output_root, '.last_build')
    self.error_js_path = os.path.join(self.output_root, 'lightning_error.js')
    self.site_config_path = os.path.join(self.content_root, 'site.yaml')
    self.include_staging = include_staging


  def get_items(self):
    """Gets all items in this site.

    Returns an array of item infos of files in the site, in the format:
    {
      "path": "relative/post.md",
      "modified": NNNNNN
    }
    """
    files = []

    # Get all files that end with .md.
    for dirpath, dirnames, filenames in os.walk(self.content_root):
      for fname in filenames:
        if fname.endswith('.md'):
          path = os.path.join(dirpath, fname)

          # Ignore files that are for staging.
          if path.startswith(self.staging_root):
            continue

          # Save the file's path and modified time.
          files.append({
            'path': path,
            'modified': os.stat(path).st_mtime
          })

    if self.include_staging:
      # Go through staging directory, replacing or adding any files that are
      # found.
      for dirpath, dirnames, filenames in os.walk(self.staging_root):
        for fname in filenames:
          if fname.endswith('.md'):
            path = os.path.join(dirpath, fname)

            # If there's a file in our list with the same unstaged path,
            # remove it.
            path_no_staging = path.replace('staging/', '')
            for i, f in enumerate(files):
              if f['path'] == path_no_staging:
                del files[i]
                break

            # Save the file's path and modified time.
            files.append({
              'path': path,
              'modified': os.stat(path).st_mtime
            })

    return files


  def get_cache(self):
    """Gets all of the files involved in the previously built version of the
    static site.
    """
    # Find the cache (it's in /.last_build).
    if not os.path.exists(self.cache_path):
      return []
    cache_file = open(self.cache_path, 'r')
    # Load the contents of the cache.
    cache = pickle.load(cache_file)
    return cache

  def save_cache(self, cache_data):
    """Pickles the cache into the proper cache root.

    Cache should be a list of objects like the following: {
      "modified": DDDD,
      "path": "/path/to-foo.md"
      "info": {
        "title": "My new post",
        ... (other parsed data except the content).
      }
    }"""
    cache_file = open(self.cache_path, 'w')
    pickle.dump(cache_data, cache_file)

  def delete_cache(self):
    """Removes the cache entirely."""
    if os.path.exists(self.cache_path):
      os.remove(self.cache_path)

  def delete_output(self):
    """Removes the contents of the output directory entirely."""
    for f in os.listdir(self.output_root):
      # Leave the .git directory intact.
      if f == '.git' or f == 'CNAME':
        continue
      file_path = os.path.join(self.output_root, f)
      self.delete_path(file_path)

  def delete_path(self, path):
    """Remove file or directory at path."""
    try:
      if os.path.isfile(path):
        os.unlink(path)
      else:
        shutil.rmtree(path)
    except Exception, e:
      print e

  def parse_date(self, date):
    """Gets the permalink parameters based on the item's info."""
    try:
      t = datetime.datetime.combine(date, DEFAULT_TIME)
    except TypeError as e:
      raise BuildException('Failed to parse date')

    return {
      'year': t.year,
      'month': t.month,
      'month_name': t.strftime('%b'),
      'day': t.day,
      'unix': int(time.mktime(t.timetuple())),
      'formatted': t.strftime(self.site_info['date_format']),
      'rfc': self.rfcformat(t)
    }

  def get_date_from_path(self, path):
    """Based on the filesystem structure (eg. blah/2014/09/20/foo-bar.md),
    extracts the date."""
    regex = '.*\/([0-9]{4})\/([0-9]{2})\/([0-9]{2})\/.*'
    match = re.match(regex, path)
    if match:
      date_tuple = map(int, match.groups())
      date = datetime.datetime(*date_tuple)
      return self.parse_date(date)

  def rfcformat(self, dt):
    """Output datetime in RFC 3339 format that is also valid ISO 8601 timestamp
    representation. From http://bugs.python.org/issue7584"""

    if dt.tzinfo is None:
      suffix = "-00:00"
    else:
      suffix = dt.strftime("%z")
      suffix = suffix[:-2] + ":" + suffix[-2:]
    return dt.strftime("%Y-%m-%dT%H:%M:%S") + suffix


  def load_template(self, name):
    """Loads a template with the given name. Returns a string with the
    template's contents."""
    # Load the special base template first.
    base = open(os.path.join(self.template_root, 'base.html')).read()
    # Then populate the contents of the base template with the desired
    # template.
    template = open(os.path.join(self.template_root, name + '.html')).read()
    return base.replace('{{template}}', template)

  def parse_item(self, path):
    """Parses markup from files like this:

      My new post
      ===========
      type: post
      posted: 2012-03-01 9:00
      slug: my-new-post
      snip: A short summary of what's written in the post.

      Just **testing**

    into an object like this:

      {
        "title": "My new post",
        "type": "post",
        "slug": "my-new-post",
        "posted": "2012-03-01 9:00",
        "posted_info": {
          "year": 2012,
          "month": 03,
          "day": 1,
          "hour": 9,
          "min": 0
        },
        "content": "Just <b>testing</b>",
        "snip": "Just <b>testing</b>",
        "permalink": "/whatever/2012/my-new-post",
        "link": "/whatever/2012/my-new-post"
      }

      Permalink is computed based on site config too. Link is the actual link
      to the content (maybe an external URL if its a link post with URL
      specified)
    """
    # Load the file.
    f = open(path, 'rU')
    lines = f.readlines()
    if len(lines) == 0:
      raise BuildException('%s: content missing' % path)
    # TODO: Perform some basic validation.
    # Extract the title.
    title = lines[0].strip()
    # Get the key: value pairs after the title.
    try:
      separator_index = lines.index('\n')
      yaml_lines = lines[2:separator_index]
      data = yaml.load(''.join(yaml_lines)) or {}
    except Exception as e:
      raise BuildException('YAML header parsing failed for %s.' % path)
    if type(data) != dict:
      raise BuildException('Expected dictionary YAML header for %s.' % path)
    # Process the rest of the post as Markdown.
    markdown_lines = lines[separator_index+1:]
    markdown_body = ''.join(markdown_lines).decode('utf-8')
    content = markdown2.markdown(markdown_body)
    # If there's no snip specified, try to parse it before the <!--more--> tag.
    snip = 'snip' in data and markdown2.markdown(data['snip']) \
                           or self.parse_snip(content)
    # If there's no snip, juse use the content, but remember that it's short.
    data['is_long'] = bool(snip)
    data['snip'] = snip or content
    # Put everything in a dict.
    data['title'] = unicode(title, 'utf8')
    data['content'] = content
    # Infer slug and type from path.
    slug = 'slug' in data and data['slug'] or self.parse_slug(path)
    type_name = 'type' in data and data['type'] or self.parse_type(path)
    data['slug'] = slug
    data['type'] = type_name
    summary = markdown_body[:markdown_body.find('.')+1]
    data['summary'] = summary
    description = 'description' in data and markdown2.markdown(data['description']) \
                                        or None
    data['description'] = description
    posted_info = None
    if 'posted' in data:
      # Parse the date if it's specified.
      posted_info = self.parse_date(data['posted'])
      data['posted_info'] = posted_info
    else:
      # Otherwise, guess the date based on the directory structure.
      posted_info = self.get_date_from_path(path)
      if posted_info:
        data['posted_info'] = posted_info
    # Compute the permalink.
    try:
      permalink = self.compute_permalink(type_name, slug, posted_info)
    except Exception:
      raise BuildException('Failed to compute permalink for %s.' % slug)
    data['permalink'] = permalink
    data['computed_link'] = 'link' in data and data['link'] or permalink
    # Compute the verb.
    verbs = self.site_info['verbs']
    data['verb'] = type_name in verbs and verbs[type_name] or 'Published'
    # Return the dict.
    return data

  def parse_slug(self, path):
    """Returns the slug."""
    if path.endswith('index.md'):
      # If it ends with index, get the second last path component.
      return path.split('/')[-2]
    else:
      # Otherwise, just get the filename.
      return path.split('/')[-1].split('.')[0]

  def parse_type(self, path):
    """Return the type. These are a bunch of defaults..."""
    if path.find('/pages/') >= 0:
      return 'page'
    elif path.find('/posts/') >= 0:
      return 'post'
    elif path.find('/drafts/') >= 0:
      return 'draft'
    elif path.find('/archives/') >= 0:
      return 'archive'
    if path.find('/links/') >= 0:
      return 'link'
    if path.find('/talks/') >= 0:
      return 'talk'
    if path.find('/projects/') >= 0:
      return 'project'

  def parse_snip(self, content):
    """Return the snippet based on the content."""
    found = content.find('<!--more-->')
    if found >= 0:
      return content[:found]

  def parse_site(self):
    """Parses a site.yaml like this:

      title: My awesome blog
      permalinks_format:
        post: {{year}}/{{slug}}
        page: {{slug}}

    into a dict like this:

      {
        "site_title": "My awesome blog"
        "site_permalinks": {
          "post": "{{year}}/{{slug}}",
          "page": "{{slug}}"
        }
      }
    """
    # Run a YAML parser and get all the stuff as a dictionary.
    f = open(self.site_config_path, 'r')
    return yaml.load(f)

  def compute_permalink(self, type_name, slug, posted_info=None):
    """Returns the permalink for the given item."""
    print slug, type_name
    formats = self.site_info['permalink_format']
    if type_name in formats:
      permalink_template = formats[type_name]
    else:
      permalink_template = formats['default']
    #print 'Using template %s' % permalink_template
    permalink_data = {'slug': slug}
    # If there's date information associated, include it in the permalink data.
    if posted_info:
      permalink_data = dict(permalink_data.items() + posted_info.items())
    return '/' + self.render_template_string(permalink_template, permalink_data)

  def render_template_string(self, template_string, data):
    template = Template(template_string)
    out = template.render(data)
    return out

  def render_template(self, filename, data):
    env = Environment(loader=FileSystemLoader(self.template_root))
    try:
      template = env.get_template(filename)
    except Exception:
      raise BuildException('Failed to find template %s.' % filename)
    try:
      out = template.render(data)
    except Exception:
      raise BuildException('Failed to render template %s.' % filename)
    return out

  def fix_broken_links(self, items):
    """
    Given a list of items for creating a list, filters all references (img src
    and a href), looking for relative pathes and rewriting the URLs to work
    from the parent page.
    """
    out = []

    for item in items:
      newItem = item.copy()
      # Parse the item content for links.
      slug = item['permalink']
      newItem['content'] = self.prefix_links(newItem['content'], slug)
      newItem['snip'] = self.prefix_links(newItem['snip'], slug)
      if 'image' in item:
        newItem['image'] = os.path.join(slug, item['image'])
      out.append(newItem)

    return out

  def prefix_links(self, content, prefix):
    links = re.findall(r'<a href="(.+?)"', content, re.DOTALL) + \
            re.findall(r'<img src="(.+?)"', content, re.DOTALL) + \
            re.findall(r'<video src="(.+?)"', content, re.DOTALL)

    # Check if they are relative.
    for link in links:
      # If it starts with a http or /, it's a relative URL.
      if not link.startswith('/') and not link.startswith('http'):
        # If they are relative, rewrite them using the slug as a prefix.
        newLink = os.path.join(prefix, link)
        if len(link) < 3:
          print 'link: "%s", content: "%s"' % (link, content)
        content = content.replace(link, newLink)

    return content


  def build(self, item):
    """Builds a single item."""
    info = item['info']
    # TODO: validate the item!
    # Combine the parsed information into template data.
    template_data = dict(info.items() + self.site_info.items())
    # Compute the final path based on permalink config.
    path = info['permalink']
    # If it's a list, also get the list parameters.
    if 'list' in info:
      limit = 'limit' in info and info['limit'] or 10
      type_filter = 'filter' in info and info['filter']
      items = self.get_list(type_filter, limit)
      # For lists, there may be broken links which were specified in relative
      # format. These links need to be rewritten with the correct pathes.
      items = self.fix_broken_links(items)
      template_data['posts'] = items

    # Get the type and load the appropriate template.
    type_name = info['type']
    template_path = type_name + '.html'
    # Evaluate the template with data.
    html = self.render_template(template_path, template_data)
    # Create the directory for the parsed HTML.
    abs_path = self.output_root + path
    dir_util.mkpath(abs_path)
    # Create the index file inside the directory.
    f = open(abs_path + '/index.html', 'w')
    f.write(html.encode('utf-8'))
    # Copy any assets that should be copied.
    self.copy_assets(item)

  def build_feed(self, item):
    """Builds an ATOM RSS feed."""
    info = item['info']
    template_data = dict(info.items() + self.site_info.items())
    # Get the list of items to build the feed from.
    limit = 'limit' in info and info['limit'] or 10
    type_filter = 'filter' in info and info['filter']
    items = self.get_list(type_filter, limit)
    items = self.fix_broken_links(items)
    template_data['posts'] = items
    # Get the current date in RFC format.
    template_data['rfc'] = self.rfcformat(datetime.datetime.now())

    # Fill template with data.
    xml = self.render_template('feed.xml', template_data)
    # Get the right path and make container directory if needed.
    abs_path = self.output_root + info['permalink'] + '.xml'
    dir_util.mkpath(os.path.dirname(abs_path))
    # Create the appropriate feed file.
    f = open(abs_path, 'w')
    f.write(xml.encode('utf-8'))

  def copy_assets(self, item):
    """If the item is an index file, and there are assets in its directory,
    copy them over."""
    if not item['path'].endswith('index.md'):
      # Not an index file, so there's nothing to do.
      return

    src = os.path.dirname(item['path'])
    dst = self.output_root + item['info']['permalink']
    #print (src, dst)
    # Copy everything in the item path to the destination path.
    dir_util.copy_tree(src, dst)
    # Remove the raw index.md.
    os.remove(dst + '/index.md')


  def archive_output(self, item):
    """Moves the output specified by this item to an archive directory."""
    path = item['info']['permalink'].strip('/')
    # Move it from the output dir into the archive dir.
    src = os.path.join(self.output_root, path)
    dst = os.path.join(self.archive_path, path)
    # Ensure target archive directory exists.
    dst_dir = os.path.dirname(dst)
    if not os.path.exists(dst_dir):
      dir_util.mkpath(dst_dir)

    # If there's already an archived thing with that name, just remove it. 
    if os.path.exists(dst):
      self.delete_path(dst)

    print 'Renaming %s to %s' % (src, dst)
    if os.path.exists(src):
      os.rename(src, dst)


  def get_list(self, type_filter, limit):
    """Gets list of posts with the given options."""
    # Only care about items with a posted attribute.
    # Doesn't matter if it was posted or not.
    out = [i['info'] for i in self.cache]# if 'posted' in i['info']]
    # Filter the list of all posts with the given parameters.
    if type_filter:
      if type(type_filter) != list:
        type_filter = [type_filter]

      out = [i for i in out if i['type'] in type_filter]
    # Order it correctly.
    def order_by_date(a, b):
      if 'posted_info' in a and 'posted_info' in b:
        return b['posted_info']['unix'] - a['posted_info']['unix']
      if 'posted_info' in a and not 'posted_info' in b:
        return 1
      if 'posted_info' in b and not 'posted_info' in a:
        return -1
      return 1
    out = sorted(out, cmp=order_by_date)
    # Limit the number of results.
    return out[:limit]

  def compare_items(self, items, cache):
    """Compares the items to the previously built items.

    Interesting things to know:
    - Was a new item added? (new path)
    - Was an item removed? (old path no longer exists)
    - Was an item updated? (same path, but updated modification time)

    Returns 3 arrays: created, updated, deleted.
    """
    # Get a list of all pathes that exist in the cache.
    cache_pathes = [c['path'] for c in cache]
    # Make it easy to look up cache mtime based on path.
    mtime_lookup = dict([(c['path'], c['modified']) for c in cache])
    # Create return arrays.
    created = []
    updated = []
    deleted = []
    # Iterate through the current items.
    for item in items:
      path = item['path']
      modified = item['modified']
      # Check if the item is in the cache.
      if path in cache_pathes:
        # If present, check if it's updated.
        cache_modified = mtime_lookup[path]
        if modified > cache_modified:
          updated.append(item)
      else:
        # If absent, it must have been just added.
        created.append(item)

      # If present, remove the checked item from the cache_pathes list.
      if path in cache_pathes:
        cache_pathes.remove(path)
        #print 'cache_pathes', cache_pathes

    # The removed items are the ones that haven't been checked.
    deleted = [i for i in cache if i['path'] in cache_pathes]

    return (created, updated, deleted)

  def copy_static(self):
    """Copies the static part of the template directory into the output
    root (only if it's not there yet)."""
    src = os.path.join(self.template_root, 'static/')
    dst = os.path.join(self.output_root, 'static/')

    #if not os.path.exists(dst):
    print 'Copying static directory from %s to %s' % (src, dst)
    dir_util.copy_tree(src, dst)

  def build_incremental(self):
    """Performs an incremental build."""
    # Get the site info.
    self.site_info = self.parse_site()
    # Get a list of all items that exist in the site.
    items = self.get_items()
    # Look in the build directory for a saved list of files from last build.
    cache = self.get_cache()
    self.cache = cache
    #print 'building incremental. cache is', len(cache)
    # Compare the two lists and get pathes that have been created/updated/etc.
    (created, updated, deleted) = self.compare_items(items, cache)
    #print 'Created', created, 'updated', updated, 'deleted', deleted

    # Copy over the static template files if necessary.
    self.copy_static()

    # If nothing changed, we're done.
    if not created and not updated and not deleted:
      return
    else:
      print 'Created %d, updated %d, deleted %d.' % \
            (len(created), len(updated), len(deleted))

    # If anything was updated, parse it and update the site cache.
    for item in updated:
      # Update permalink if necessary.
      path = item['path']
      info = self.parse_item(path)
      item['info'] = info
      # Remove this item in the cache.
      cache[:] = [i for i in cache if i['path'] != path]
      # Update the file entry.
      cache.append(item)

    # If something was created, parse it and add to cache.
    for item in created:
      path = item['path']
      info = self.parse_item(path)
      item['info'] = info
      # Add to cache.
      cache.append(item)

    # If something was deleted, archive from output directory.
    for item in deleted:
      if item['info']['type'] == 'feed':
        continue
      self.archive_output(item)
      # Remove this item in the cache.
      cache[:] = [i for i in cache if i['path'] != item['path']]

    # Get all of the lists from the updated cache.
    lists = [i for i in cache if 'list' in i['info']]
    # Distinguish between single and lists posts.
    singles = [i for i in created + updated if 'list' not in i['info']]

    # Build all of the single files.
    for info in singles:
      self.build(info)
    # Build the lists.
    for info in lists:
      # Special logic for building feeds.
      if info['info']['type'] == 'feed':
        self.build_feed(info)
      else:
        self.build(info)

    # Write out the new cache to disk.
    self.save_cache(cache)

    # Remove the error file if it exists.
    if os.path.exists(self.error_js_path):
      os.remove(self.error_js_path)

  def get_list_pathes(self):
    out = []
    cache = self.get_cache()
    lists = [i for i in cache if 'list' in i['info']]
    for item in lists:
      item_info = item['info']
      if 'permalink' in item_info:
        item_type = item_info['type']
        permalink = item_info['permalink']
        # Ignore image_index pages.
        if item_type == 'image_index':
          continue
        path = permalink
        if item_type == 'feed':
          path = permalink + '.xml'
        elif item_type == 'index':
          path = permalink + 'index.html'
        elif item_type == 'archive':
          path = os.path.join(permalink, 'index.html')
        out.append(path)
    return out

  def handle_error(self, e):
    # Write out the lightning_error.json file to the root of the build.
    error_data = 'alert("%s");' % str(e)
    f = open(self.error_js_path, 'w')
    f.write(error_data)

    # Print out the exception.
    print 'Error: %s' % e


class Command:
  BUILD = 1     # Incremental build.
  REBUILD = 2
  PREVIEW = 3
  WATCH = 4
  PURGE = 6
  DEPLOY = 7

command_map = {
  'build': Command.BUILD,
  'rebuild': Command.REBUILD,
  'preview': Command.PREVIEW,
  'watch': Command.WATCH,
  'purge': Command.PURGE,
  'deploy': Command.DEPLOY,
}

def error(msg):
  print 'Error:', msg
  sys.exit(1)

class BuildHandler(FileSystemEventHandler):

  def on_any_event(self, event):
    print event.src_path, event.event_type
    # Only care about .md changes.
    if not event.src_path.endswith('.md'):
      return
    builder = SiteBuilder()
    # Uncomment the following to do a full rebuild every time.
    #builder.delete_output();
    try:
      builder.build_incremental()
    except BuildException as e:
      return builder.handle_error(e)


class S3Uploader:

  def __init__(self, local_directory, s3_bucket): 
    self.local_directory = local_directory
    self.s3_bucket = s3_bucket

  def upload(self, pathes=None):
    keys = []
    # Get the list of files to upload.
    if pathes:
      for path in pathes:
        # Build up the key to upload based on this permalink.
        key = path.strip('/')
        keys.append(key)
    else:
      # Upload directory recursively.
      for path, dir, files in os.walk(self.local_directory):
        for filename in files:
          filepath = os.path.join(path, filename)
          key = os.path.relpath(os.path.join(path, filename), self.local_directory)
          keys.append(key)

    self.upload_keys(keys)

  def upload_keys(self, keys):
    # Establish connection.
    c = boto.connect_s3()

    # Get or create bucket with proper permissions.
    try:
      b = c.get_bucket(self.s3_bucket)
    except Exception:
      print 'No bucket %s found.' % self.s3_bucket
      b = c.create_bucket(self.s3_bucket)
    k = Key(b)

    for key in keys:
      filepath = os.path.join(self.local_directory, key.strip('/'))
      print filepath
      # Check if a file with this key already exists.
      remote_key = b.get_key(key)
      if remote_key:
        # If it does, check if the files are identical.
        # Note: S3 stores uses MD5 for their ETags. http://goo.gl/rGVXM
        local_md5 = k.compute_md5(open(filepath))[0]
        remote_md5 = remote_key.etag.strip('"')
        if local_md5 == remote_md5:
          print '%s is already uploaded. Skipping.' % key
          # If the files are identical, do nothing.
          continue;

      # Otherwise if no file exists, or if they are different, upload.
      k.key = key
      k.set_contents_from_filename(filepath)
      b.set_acl('public-read', k.key)
      print 'Uploading %s.' % k.key

  def verify_path(self, permalink):
    path = os.path.join(self.local_directory, permalink.strip('/'))
    return os.path.exists(path)


def deploy():
  builder = SiteBuilder(include_staging=False)
  if not builder.deploy:
    error('Deploy bucket is not set')
  # Build incrementally to make sure we're up to date.
  try:
    builder.build_incremental()
  except BuildException as e:
    return builder.handle_error(e)
  print 'Deploying to %s.' % builder.deploy
  # Create an S3 uploader.
  uploader = S3Uploader(builder.output_root, builder.deploy)
  # Upload.
  uploader.upload()


class LightningWatchThread(threading.Thread):
  def __init__(self):
    super(LightningWatchThread, self).__init__()
    self._stop = threading.Event()

  def run(self):
    observer = Observer()
    observer.schedule(BuildHandler(), builder.content_root, recursive=True)
    # Uncomment following line for template development.
    observer.schedule(BuildHandler(), builder.template_root, recursive=True)
    observer.start()

    while not self._stop.isSet():
      time.sleep(1)

    observer.stop()
    observer.join()

  def stop(self):
    print 'Stopping LightningWatchThread'
    self._stop.set()

if __name__ == '__main__':
  if len(sys.argv) <= 1:
    valid_commands = ', '.join(command_map.keys())
    print 'Valid commands: ', valid_commands
    error("Specify at least one command.")
  command_name = sys.argv[1] or 'build'
  try:
    command = command_map[command_name]
  except Exception:
    error("Unknown command \"%s\"" % command_name)

  builder = SiteBuilder()
  if command is Command.BUILD:
    try:
      builder.build_incremental()
    except BuildException as e:
      builder.handle_error(e)

  elif command is Command.PURGE:
    builder.delete_output();

  elif command is Command.REBUILD:
    builder.delete_output();
    try:
      builder.build_incremental()
    except BuildException as e:
      builder.handle_error(e)

  elif command is Command.PREVIEW:
    # On a separate thread, start a livereload server.
    lightning_thread = LightningWatchThread()
    lightning_thread.start()

    def signal_handler(signal, frame):
      print('You pressed Ctrl+C!')
      lightning_thread.stop();
      sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)

    # On main thread, run livereload
    server = Server()
    server.serve(root=builder.output_root, port=PREVIEW_PORT, host=PREVIEW_ADDR)


  elif command is Command.DEPLOY:
    # Build the site without staging.
    builder = SiteBuilder(include_staging=False)
    builder.delete_output();
    try:
      builder.build_incremental()
    except BuildException as e:
      builder.handle_error(e)
